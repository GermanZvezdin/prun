                                Job description
--------------------------------------------------------------------------------

Job description is represented in the JSON formatted file. So job description
is a set of key-value pairs, where key is a certain job property. Master expects
that job description files are in the "prun/jobs" directory. So this is a good
idea to share this directory in the network for putting job descriptions from
other computers. Following job properties must be set in each job description:

- script
Path to the script file. This script file will be executed on worker computers.
If path is relative, then master will look for the file in the "prun/jobs"
directory. "script" property is closely related to "send_script" property.
If "send_script" value is false, then path must point to a script file on the
remote worker computer, thus path must be absolute.

- send_script
Whether master sends content of a script file or not. It is possible to send
script file content to workers without sharing this script file across network.
If file is large, then there is a problem of sending too much data, devouring
network bandwidth.
Also, if script file size is larger than 512 Kb, then it is necessary to set
"send_script" value to false, because of some worker's internal limitations.

- language
Script language might be one of the following: python, ruby, js, java, shell.

- priority
The smaller the integer value of the "priority" option, the higher the priority
of the job. Job with a higher priority will be scheduled before another job with
a lower priority.

- job_timeout
Job timeout in seconds. If scheduled by the master job is not completed in a
period of job_timeout seconds, then master sends "stop job" command to all job
executing workers.

- queue_timeout
All incoming master jobs are queued. If master doesn't schedule the job in a
period of queue_timeout seconds, then this job will be removed from the job
queue.

- task_timeout
Task timeout in seconds. This timeout means task execution timeout. If the
running process is not completed in a period of task_timeout seconds, then
supervising worker process will terminate it immediately.

NOTE: Negative timeout value means infinite timeout.

- max_failed_nodes
Maximum number of failed nodes whereupon job will be stopped on all workers.
Negative value means no limits.

- num_execution
Integer number of planned job executions.
If the value is negative:
a) if exec_unit_type is "cpu" (default) then the number of planned job executions
equals to the total number of cluster CPUs.
b) otherwise, if hosts list is not empty, then the number of planned job
executions equals to the size of hosts list, otherwise it equals to the number of
total available worker hosts.

- max_cluster_instances
Maximum number of simultaneously running instances of the job in the cluster.
Negative value means no limits.

- max_worker_instances
Maximum number of simultaneously running instances of the job in the single
worker machine. Negative value means no limits.

- exclusive
Exclusive job couldn't be executed simultaneously with any other job.

- no_reschedule
If the value is false, then master reassigns job from failed worker to any other
free worker, otherwise job won't be reassigned.

- max_exec_at_worker (optional)
Maximum number of job execution at a single worker.
Negative value means no limits (default).

- exec_unit_type (optional)
The value of this parameter jointly used with num_execution parameter.
If the value is "cpu", then cluster unit is a set of CPUs (default).
If the value is "host" then cluster unit is a single cluster worker.

- hosts (optional)
List of hosts that are allowed to execute job. Empty list means no limits.

- groups (optional)
List of groups of hosts, that are allowed to execute job. Empty list means no
limits.

- cron (optional)
String like "*/5 * * * 0", which means 'run this job every 5 minutes on Sunday'.
Use 'man 5 crontab' for more inforamtion.
NB: if this field is defined, then field 'name' should be also defined.

- name (optional)
Unique name of job or meta job that identifies definitely this job or meta job.

                                Job requirements
--------------------------------------------------------------------------------

In general, to create a job, do one of the following: write script that will
perform the actual job itself or will start another program that will perform
the job. Supported scripting languages are presented in the job description
section. Master expects that script files are in the "prun/jobs" directory.

Master will execute a job multiple times across cluster machines. Each
particularly running job is called a "task". So the "job" is a set of "tasks".
Number of tasks depends on job properties "num_execution", "max_cluster_instances"
and the total number of cluster CPUs (for more detailed information see job
description section).
When Worker starts your script (aka "task"), it passes three input parameters:
"taskId", "numTasks" and "jobId". The meaning of first two parameters are close
to MPI's MPI_Comm_rank and MPI_Comm_size respectively. The value of "numTasks"
parameter is the number of planned job executions. The value of "taskId"
parameter is an integer identifier of a currently running task, that lies in
the half-closed interval [0, "numTasks"). The value of "jobId" parameter is an
integer identifier of currently running job. Note that "jobId" is a unique
identifier only in a context of running Master session. So, after restart of a
Master process, the value of "jobId" parameter for newly running tasks may be
equal to such one "jobId" value of previously running jobs.

Each running job returns its completion status to a supervising Worker process.
If job is written in one of supported script languages (e.g. Python or Ruby),
then it should throw an exception to return its negative completion status,
while returning positive completion status doesn't require any additional action.
Note that shell scripts or running binary executables must return its exit code
as a completion status, while other script languages shouldn't call "exit"
command, because of its execution within a context of supervising script.
As usually, zero exit code means normal completion status, while non-zero value
would be interpreted by a Master as job failure, so Master possibly reassigns
that job to any other node as it mentioned above in the job description section.

                           Job dependency graph. Metajobs
--------------------------------------------------------------------------------

Job dependency graph is a directed acyclic graph (DAG) representing dependencies
of several jobs towards each other. It is also called "metajob" in context of
Prun. Master takes that job dependencies from the .meta file, that is written in
JSON format and describes job dependency graph via adjacency list.
Master expects that .meta files are in the "prun/jobs" directory.

                        Workers and worker groups management
--------------------------------------------------------------------------------

Workers are identified by a hostname or host ip. Set of workers is called a
worker group. Workers always belong to some worker group. Master reads "hosts"
file during its initialization. This "hosts" file is in the "prun" directory, it
contains list of worker groups. Master expects that worker group files are also
in the "prun" directory. So, if you are deployed workers, you should create
text file, which contains hostnames or IP's of worker machines (line by line).
This text file must be named as the name of worker group. Then you just append
this group name to the end of "hosts" file.
Also it is possible to manually manage workers, using admin tool (see
administration tool section). It is possible to add or delete workers, worker
groups on the fly.
So Master must know about Workers, but Workers should not know anything about
Master, therefore connecting Master with Workers consists only of steps
described above.
Master takes into account computer specifications. Computer with lots of RAM and
free CPU's is more likely to get a job.

                             Worker configuration
--------------------------------------------------------------------------------

Worker's configuration described in the "prun/worker.cfg" file. This file is a
set of key-value pairs, written in JSON format. It contains pathes to language
interpreters ("python", "ruby", "shell", "js") and compilers ("javac"). Other
config settings:

- pidfile
Name of a pidfile. Worker writes its PID into this file during initialization
phase. It is necessary for recognition of silent Worker failures.

- completion_ping_delay
After job completion, Worker puts job completion status into the job completion
table. If this table is not empty, then Worker periodically sends signals to
Master. Worker sends signal every "completion_ping_delay" seconds, until Master
connects to Worker to get job completion status.

- ipv6
Setting this parameter value to true leads to using of IPv6 protocol.

- port (optional, default = 5555)

- ping_port (optional, default = 5554)

- master_ping_port (optional, default = 5553)

                             Master configuration
--------------------------------------------------------------------------------

Master's configuration described in the "prun/master.cfg" file. This file is a
set of key-value pairs, written in JSON format. Settings description:

- pidfile
Name of a pidfile. Master writes its PID into this file during initialization
phase. It is necessary for recognition of silent Master failures.

- jobs_path
Relative or absolute path to the directory, where Master will search for job/
metajob files and script files.

- heartbeat_delay
Master sends heartbeat signal to Workers every "heartbeat_delay" seconds.

- heartbeat_max_droped
If Worker doesn't respond for more than the value of "heartbeat_max_droped"
heartbeat signals, then Master decides that Worker is not available.
For example, if network failed between Worker and Master during more than
"heartbeat_delay" * "heartbeat_max_droped" seconds, then Master should reassign
Worker jobs to any other available Worker because of network failure. So, such
events like network failure, Worker sudden termination and some other exceptional
cases lead to Master's decision of Worker's failure.

- num_ping_receiver_thread
The number of ping (heartbeat) receiving threads.

- num_job_send_thread
The number of job sending threads. Each thread waiting for a new planned job,
prepares message for Worker and initiates asynchronous tcp connection to Worker
for sending this message, containing job description.

- num_result_getter_thread
The number of threads that perform obtaining completion status of achieved jobs
from Workers.

- num_command_send_thread
The number of command sending threads. List of commands described in the
Administration tool section.

- max_simult_sending_jobs
The maximum number of simultaneously sending jobs.

- max_simult_result_getters
The maximum number of simultaneously job completion status getters.

- max_simult_command_send
The maximum number of simultaneously sending commands.

- ipv6
Setting this parameter value to true leads to using of IPv6 protocol. Note,
that all Workers must have the same value of "ipv6" settings in their config
files as the Master's "ipv6" value.

- masterdb
IP or hostname of masterdb service

- node_port (optional, default = 5555)

- node_ping_port (optional, default = 5554)

- master_ping_port (optional, default = 5553)

- master_admin_port (optional, default = 5557)

- masterdb_port (optional, default = 5559)

                             Masterdb configuration
--------------------------------------------------------------------------------

Masterdb's configuration described in the "prun/masterdb.cfg" file. This file is
a set of key-value pairs, written in JSON format. Settings description:

- pidfile
Name of a pidfile. Masterdb writes its PID into this file during initialization
phase. It is necessary for recognition of silent Masterdb failures.

- db_path
Path to the jobs database directory. Used in configuration with LevelDB library.

- ipv6
Setting this parameter value to true leads to using of IPv6 protocol. Note, that
Masterdb must have the same value of "ipv6" settings as the Master's "ipv6".

- port (optional, default = 5559)

                             Network configuration
--------------------------------------------------------------------------------

Master default listen ports: 5557 (tcp), 5553(udp)
Masterdb default listen ports: 5559 (tcp)
Worker default listen ports: 5555 (tcp), 5554(udp)

                                 Logs
--------------------------------------------------------------------------------

If Master or Worker is started as a daemon, then it writes logs to system log,
otherwise it writes logs to standard output (console).

                            Administration tool
--------------------------------------------------------------------------------

Prun has simple command-line administration tool. Admin tool connects to Master
and then prompts user to write some command. To run admin tool type following
commands:

> cd ~/prun                 # cd to the directory containing prun
> ./prun master_id     # master_id is Master's ip or hostname
                            # or empty, if Master started on localhost

To display list of commands, type "help" and press return. Also, it is possible
to push set of commands as a command-line argument, for example:

> ./prun -c "run my_job.meta; sleep 2; stopall"

Command delimeter is ';'. There are special "sleep" command, which forces admin
tool to sleep for N seconds, before execution of following commands.